{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\like}{\\mathcal{L}}$\n",
    "\n",
    "There are two primary ways to use probability models: prediction and inference.\n",
    "\n",
    "Given what we know to be true about an experimental setup, we can make predictions about what we expect to see in an upcoming trial. If $A$ is an event resulting from an experiment and $p$ is a parameter of the probability function defined for these experimental outcomes, we can write these expectations or predictions as:\n",
    "\n",
    "$$P\\ (A\\ \\given[\\big]\\ p)$$\n",
    "\n",
    "Read as \"the probability of $A$ given $p$\", this conditional probability tells us what to expect about the outcomes of our experiment, given knowledge of the underlying probability model. Thus, it is a function of $A$ given a value for $p$ (and the model itself).\n",
    "\n",
    "However, we might also wish to ask what we can learn about $p$, given outcomes of trials that have already been observed. This is the purview of the likelihood. Likelihoods are functions of parameters (or hypotheses, more generally) given some observations. The likelihood of a parameter value is defined as:\n",
    "\n",
    "$$\\like\\ (p\\ ;\\ A)\\ =\\ P\\ (A\\ \\given[\\big]\\ p)$$\n",
    "\n",
    "This is the same probability statement we saw above. However, in this context we are considering the outcome ($A$) to be fixed and we're interested in learning about $p$. Note that the likelihood is sometimes written in several different ways:\n",
    "\n",
    "$$\\like\\ (p\\ ;\\ A)$$ $$\\like\\ (p)$$ $$\\like\\ (p\\ \\given[\\big]\\ A)$$\n",
    "\n",
    "$P(A|p)$ corresponds to a probability when $A$ is discrete or a probability density when $A$ is continuous. Since likelihoods are conditioned on an observed outcome $A$, which is either discrete or continuous, we do not need to worry about this distinction. More precisely, likelihoods are proportional to $P(A|p)$, with an arbitrary constant of proportionality.\n",
    "\n",
    "There are some very important distinctions between likelihoods and probabilities. First, likelihoods do NOT sum (or integrate) to 1 over all possible values of $p$. Therefore, the area under a likelihood curve is not meaningful, as it is for probability.\n",
    "\n",
    "Second, it does not make sense to compare likelihoods across different $A$ (i.e., compare likelihoods for different datasets). For instance, smaller numbers of observations generally produce higher values of $P(A|p)$ and $L(p;\\ A)$, because the total sample space is smaller.\n",
    "\n",
    "Comparisons of likelihoods, often depicted as likelihood curves, provide useful information about different possible values of $p$. When we are interested in comparing different hypotheses ($H_1$ and $H_2$), the likelihood ratio is often used:\n",
    "\n",
    "$$\\frac{\\like(H_1\\ ;\\ A)}{\\like(H_2\\ ;\\ A)}=\\frac{P(A\\ \\given[\\big]\\ H_1)}{P(A\\ \\given[\\big]\\ H_2)}$$\n",
    "\n",
    "Let's start by defining the outcome of a coin-flipping experiment with 5 flips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flips = [\"H\",\"T\",\"T\",\"T\",\"H\"]\n",
    "print(flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count up the number of heads that occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numHeads = 0\n",
    "for fl in flips:\n",
    "    if (fl == \"H\"):\n",
    "        numHeads += 1    # This is equivalent to numHeads = numHeads + 1\n",
    "print(numHeads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way to calculate that same tally in a very Python-ique way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numHeads2 = sum([1 for fl in flips if fl == \"H\"])\n",
    "print(numHeads2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate our first likelihood in the case where we have a trick coin with two heads. In the language of a binomial distribution, this would correspond to $p=1$ (only heads, no tails). So, we're interested in calculating\n",
    "\n",
    "$$\\like(p=1) = \\like(p=1\\ ;\\ H=2,\\ n=5) = P(H=2\\ \\given[\\big]\\ p=1,\\ n=5)$$\n",
    "\n",
    "Here, we're defining $H$ to be a random variable that corresponds to the number of heads. $H$ then has a binomial distribution with 5 flips ($n=5$) and, in this case, $p=1$:\n",
    "\n",
    "$$H \\sim Bin(5,1)$$\n",
    "\n",
    "The likelihood of $p=1$, which is just the binomial probability for the observed number of heads, can be written:\n",
    "\n",
    "$$\\like(p=1)\\ =\\ P(H=2\\ \\given[\\big]\\ p=1,\\ n=5)\\ \\\\\n",
    "={5 \\choose 2}\\ 1^2\\ 0^3\\\\\n",
    "=0$$\n",
    "\n",
    "What is this likelihood? Does this make sense?\n",
    "\n",
    "Now let's try the same calculation for $p=0.3$.\n",
    "\n",
    "$$\\like(p=0.3)\\ =\\ P(H=2\\ \\given[\\big]\\ p=0.3,\\ n=5)\\ \\\\\n",
    "={5 \\choose 2}\\ 0.3^2\\ (1-0.3)^3\\\\\n",
    "={5 \\choose 2}\\ 0.3^2\\ 0.7^3\\\\\n",
    "=0.3087$$\n",
    "\n",
    "In the cell below, paste the code you wrote for Assignment 2 to calculate the PMF of a binomial distribution. Does it give the same answer for these two values of p?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we could also take advantage of the fact that we're modeling the outcomes of our experiment with a binomial, which is a common statistical distribution. The scipy library contains built-in functions to deal with the binomial distribution. In the following line, we're specifically drawing from the stats module of the scipy library and importing the binom (binomial) class. More information on this class is available here:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that our previous likelihood calculations, including your code, were correct by comparing them to scipy's binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "like_1_sp = binom.pmf(2,5,1)\n",
    "print(like_1_sp)\n",
    "like_03_sp = binom.pmf(2,5,0.3)\n",
    "print(like_03_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's automate the calculation of likelihoods for a whole series of values of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first line uses list comprehension to produce values of p: 0, 0.01, 0.02, ..., 1\n",
    "# range() only produces integers, so we divide by the max value to get values b/w 0 and 1.\n",
    "p_vals = [i/100 for i in range(0,101,1)]\n",
    "\n",
    "# Now we calculate the binomial PMF for each value of p in the p_vals list\n",
    "likes = [binom.pmf(numHeads,len(flips),p) for p in p_vals]\n",
    "\n",
    "# Plotting likelihood curve using the matplotlib library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(p_vals, likes, linewidth=2, label='')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_ylabel('Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this again, but pretend we had 10 times as much data, keeping the proportion of heads constant at 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same as above\n",
    "p_vals = [i/100 for i in range(0,101,1)]\n",
    "\n",
    "# Like above, but with 10 times as many flips and heads\n",
    "likes_10x = [binom.pmf(numHeads*10,len(flips)*10,p) for p in p_vals]\n",
    "\n",
    "# Plotting likelihood curve using the matplotlib library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(p_vals, likes_10x, linewidth=2, label='')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_ylabel('Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these plots, what are some immediate things you notice? How do the shapes compare? What about their modes? What about the maximum value of the likelihood (hint: look at the y-axis scales)?\n",
    "\n",
    "Now let's try comparing the likelihoods for two values of $p$ by calculating their ratio. As a place to start, let's use $p=0.4$ and $p=0.6$. For our original dataset\n",
    "\n",
    "$$\\frac{\\like(p\\ =\\ 0.4)}{\\like(p\\ =\\ 0.6)}\\ =\\ \\frac{{5 \\choose 2}\\ 0.4^2\\ (1-0.4)^3}{{5 \\choose 2}\\ 0.6^2\\ (1-0.6)^3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(binom.pmf(2,5,0.4)/binom.pmf(2,5,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"10x\" dataset,\n",
    "\n",
    "$$\\frac{\\like(p\\ =\\ 0.4)}{\\like(p\\ =\\ 0.6)}\\ =\\ \\frac{{50 \\choose 20}\\ 0.4^{20}\\ (1-0.4)^{30}}{{50 \\choose 20}\\ 0.6^{20}\\ (1-0.6)^{30}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(binom.pmf(20,50,0.4)/binom.pmf(20,50,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would these two datasets affect whether we accept 0.4 or 0.6 as reasonable values for $p$?\n",
    "\n",
    "How have we been thinking about our data and what we observed? We can also calculate the probability of our exact dataset (not just the number of heads and tails, but also the precise sequence) like this:\n",
    "\n",
    "$$P(D)\\ =\\ P(D[0]\\ \\given[\\big]\\ p=1)\\ \\cdot\\ P(D[1]\\ \\given[\\big]\\ p=1)\\ \\cdot\\ \\ldots\\ \\cdot\\ P(D[n-1]\\ \\given[\\big]\\ p=1)$$\n",
    "\n",
    "Note that $D$ represents our data and I'm using a Python-like list notation here to indicate individual flips in order to highlight the connections between the code and the math. This is not formal mathematical notation.\n",
    "\n",
    "Here's one example of code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=0.9                  # Try running this same code block with different values for p\n",
    "\n",
    "like_1 = 1             # Since likelihood calculations involve multiplication,\n",
    "for fl in flips:       #    always start with 1.\n",
    "    if (fl == \"H\"):\n",
    "        like_1 *= p\n",
    "    else:\n",
    "        like_1 *= 1-p\n",
    "print(like_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this likelihood? Does this make sense?\n",
    "\n",
    "Since we'll want to calculate more likelihoods in the future, let's just make this a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def like(data,prob):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood for a coin-flipping experiment with \n",
    "    observed flips and a chosen value of p. Expects data to be a list of \n",
    "    Hs and Ts - capitalized!\n",
    "    \"\"\"\n",
    "    likeVal = 1\n",
    "    for d in data:\n",
    "        if (d == \"H\"):\n",
    "            likeVal *= prob      # Heads are \"successes\".\n",
    "        elif (d == \"T\"):\n",
    "            likeVal *= (1-prob)  # Tails are \"failures\".\n",
    "        else:                    # Error catching for improper data.\n",
    "            raise ValueError(\"Data contains something other than H or T.\")\n",
    "    return likeVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think about our data this way, as a precise sequence, how does it affect the likelihood ratios comparing $p=0.4$ and $p=0.6$?\n",
    "\n",
    "For the original dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(like(flips,0.4)/like(flips,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"10x\" dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(like(flips*10,0.4)/like(flips*10,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these ratios compare to the likelihood ratios we calculated with the binomial? Does this make sense? How do the raw values of the likelihoods compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Binomial likelihood for p=0.4: \"+str(binom.pmf(2,5,0.4)))\n",
    "print(\"Sequence likelihood for p=0.4: \"+str(like(flips,0.4)))\n",
    "print()  # Spacer line\n",
    "print(\"Binomial likelihood for p=0.6: \"+str(binom.pmf(2,5,0.6)))\n",
    "print(\"Sequence likelihood for p=0.6: \"+str(like(flips,0.6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on maximum likelihood estimation, look over this article on Wikipedia: \n",
    "\n",
    "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "\n",
    "For the definitive reference, see Anthony Edward's book *Likelihood*:\n",
    "\n",
    "Edwards AWF (1972) *Likelihood*. Cambridge University Press. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "\n",
    "### Assignment 2b\n",
    "\n",
    "Now you get to write some code on your own to do two things:\n",
    "\n",
    "(1) Climb up a likelihood surface and return the value of the parameter that maximizes the likelihood (i.e., the maximum likelihood or ML value).\n",
    "\n",
    "(2) Create and conduct a simulation to determine the ratio of likelihoods between an ML and true parameter values. Due to the variation inherent to a stochastic process, the ML value is often not the same as the true parameter value. It is useful to know how much smaller we might expect the likelihood of the true value to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ML Hill Climber\n",
    "\n",
    "Sometimes it will not be feasible or efficient to calculate the likelihoods for every\n",
    "value of a parameter in which we're interested. Also, that approach can lead to large\n",
    "gaps between relevant values of the parameter. Instead, we'd like to have a 'hill\n",
    "climbing' function that starts with some arbitrary value of the parameter and finds\n",
    "values with progressively better likelihood scores. This is an ML optimization\n",
    "function. There has been a lot of work on the best way to do this. We're going to try\n",
    "a fairly simple approach that should still work pretty well, as long as our likelihood \n",
    "surface is unimodal (has just one peak). Our algorithm will be:\n",
    "\n",
    "1. Calculate the likelihood for our starting parameter value (we'll call this *pCurr*)\n",
    "<br><br>\n",
    "2. Calculate likelihoods for the two parameter values above (*pUp*) and below (*pDown*)\n",
    "our current value by some amount (*diff*). So, *pUp*=*pCurr*+*diff* and *pDown*=*pCurr*-*diff*. To start, set *diff*=0.1, although it would be nice to allow this initial value to be set as an argument of our optimization function.\n",
    "<br><br>\n",
    "3. If either *pUp* or *pDown* has a better likelihood than *pCurr*, change *pCurr* to this\n",
    "value. Then repeat (1)-(3) until *pCurr* has a higher likelihood than both *pUp* and\n",
    "*pDown*.\n",
    "<br><br>\n",
    "4. Once *L*(*pCurr*) > *L*(*pUp*) and *L*(*pCurr*) > *L*(*pDown*), reduce *diff* by 1/2. Then repeat\n",
    "(1)-(3).\n",
    "<br><br>\n",
    "5. Repeat (1)-(4) until *diff* is less than some threshold (say, 0.001).\n",
    "<br><br>\n",
    "6. Return the final optimized parameter value.\n",
    "<br>\n",
    "\n",
    "Write a function that takes some starting *p* value and observed data (k,n) for a\n",
    "binomial as its arguments and returns the ML value for *p*. To write this function, you will probably want to use while loops. The structure of\n",
    "these loops is\n",
    "\n",
    "```python\n",
    "while (someCondition):\n",
    "    code line 1 inside loop\n",
    "    code line 2 inside loop\n",
    "```\n",
    "    \n",
    "As long as the condition remains True, the loop will continue executing. If the\n",
    "condition isn't met (`someCondition == False`) when the loop is first encountered, the \n",
    "code inside will never execute.\n",
    "<br><br>\n",
    "If you understand recursion, you can use it to save some lines in this code, but it's\n",
    "not necessary to create a working function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Simulation of Likelihood Ratio Cutoff\n",
    "\n",
    "In the exercise above, you thought about an intuitive cutoff for likelihood ratio\n",
    "scores that would suggest a value of *p* had a likelihood too low to reasonably be considered. Now, we will empirically determine one way to construct such an interval. To do \n",
    "so, we will ask how far away from the true value of a parameter the ML estimate \n",
    "might stray. Use this procedure:\n",
    "<br>\n",
    "1. Start with a known value for *p*.\n",
    "<br><br>\n",
    "2.  Simulate at least 100 datasets.\n",
    "<br><br>\n",
    "3.  Find ML parameter estimates of *p* for each simulation, using the hill climber function you wrote above.\n",
    "<br><br>\n",
    "4.  Calculate likelihood ratios comparing the true *p* to ML estimates.\n",
    "<br>\n",
    "\n",
    "By following this procedure, you will be constructing a null distribution of\n",
    "likelihood ratios that might be expected if the value of *p* you picked in (1)\n",
    "was true. Note that the ML values for these replicates are very often greater than\n",
    "L(true value).\n",
    "\n",
    "Once you have this distribution, find the likelihood ratio cutoff you need to ensure that the probability of seeing an LR score that big or greater is <= 5%. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
